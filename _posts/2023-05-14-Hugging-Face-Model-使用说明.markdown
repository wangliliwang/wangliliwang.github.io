---
layout: post
title:  "ğŸ¤— Hugging Face Model ä½¿ç”¨è¯´æ˜"
date:   2023-05-14 14:11:49 +0800
categories: Computer-Science
tags: ["AI"]
toc: true
language: chinese
sidebar: true
author: Wang Li
---

æœ¬æ–‡çš„ä¸»è¦å†…å®¹ç”¨ `transformers` è¿™ä¸ª Python åŒ…ï¼Œè¯´æ˜ model å¦‚ä½•äº§ç”Ÿã€ä½¿ç”¨ï¼Œä»¥åŠ Hugging Face model repo ä¸­çš„æ–‡ä»¶ã€‚
æœ¬æ–‡çš„ model æŒ‡çš„æ˜¯ transformer æ¶æ„çš„ modelï¼Œæ­¤ç±» model é€šå¸¸ç”¨äº NLP ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬åˆ†ç±»ã€æ–‡æœ¬ç”Ÿæˆç­‰ã€‚

## Model æ˜¯ä»€ä¹ˆï¼Ÿ

model æ˜¯ä¸€ä¸ªå¯ä»¥å¤„ç†è¾“å…¥æ•°æ®çš„å‡½æ•°ï¼Œè¾“å…¥æ•°æ®å¯ä»¥æ˜¯å›¾ç‰‡ã€æ–‡æœ¬ã€éŸ³é¢‘ç­‰ï¼Œè¾“å‡ºæ•°æ®å¯ä»¥æ˜¯åˆ†ç±»ã€å›å½’ã€æ–‡æœ¬ç”Ÿæˆç­‰ã€‚
model çš„å‚æ•°å€¼æ˜¯åœ¨è®­ç»ƒé˜¶æ®µå­¦ä¹ åˆ°çš„ï¼Œè¿™äº›å‚æ•°å¯ä»¥è¢«ä¿å­˜ï¼Œä»¥ä¾¿åœ¨åç»­é˜¶æ®µä½¿ç”¨ã€‚

æœ‰è¯æ®è¡¨æ˜ï¼Œè®­ç»ƒæ•°æ®é‡è¶Šå¤§ã€model å‚æ•°é‡è¶Šå¤§ï¼Œæ•ˆæœè¶Šå¥½ï¼Œæ‰€ä»¥ç°åœ¨æœ€æµè¡Œçš„ model æ˜¯ LLMï¼ˆLarge Language Modelï¼‰ã€‚æ‰€ä»¥å½“å‰ model çš„äº§ç”Ÿä¸ºä¸¤ä¸ªé˜¶æ®µï¼š
1. pre-trainï¼šä½¿ç”¨å¤§é‡çš„æ•°æ®ï¼Œè¿›è¡Œè‡ªç›‘ç£è®­ç»ƒã€‚è¿™æ ·è®­ç»ƒå‡ºæ¥çš„ modelï¼Œå¯¹ç‰¹å®šä»»åŠ¡å¸®åŠ©ä¸å¤§ã€‚ä½†æ˜¯å¯ä»¥åœ¨ä¸‹ä¸€ä¸ªé˜¶æ®µè¢« transfer æˆç‰¹å®šä»»åŠ¡çš„ modelã€‚ 
2. transferï¼šä½¿ç”¨é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„æ•°æ®ï¼ˆæ•°æ®è§„æ¨¡ä¸€èˆ¬è¾ƒ pre-train é˜¶æ®µå°å¾—å¤šï¼‰è®­ç»ƒä¸Šä¸€é˜¶æ®µäº§ç”Ÿçš„ modelï¼Œè¿™ä¸€é˜¶æ®µæ˜¯ç›‘ç£è®­ç»ƒã€‚æ­¤é˜¶æ®µä¹Ÿç§°ä¸º fine-tuneã€‚

ä¹‹æ‰€ä»¥åˆ†æˆä¸¤ä¸ªé˜¶æ®µï¼Œè€Œä¸æ˜¯ç›´æ¥é’ˆå¯¹ç‰¹å®šä»»åŠ¡è®­ç»ƒï¼Œæ­£æ˜¯å› ä¸ºä¸Šé¢çš„è®¾è®¡å¯ä»¥å¤ç”¨éœ€è¦å¤§é‡æ•°æ®è®­ç»ƒçš„ pre-train é˜¶æ®µã€‚

### ä½¿ç”¨ç©ºç™½ model

ç©ºç™½çš„ model åªåŒ…å«äº†æ¨¡å‹çš„ç»“æ„ï¼Œå‚æ•°æ˜¯éšæœºç”Ÿæˆçš„ï¼Œè¾“å‡ºä¹Ÿæ˜¯éšæœºçš„ï¼Œæ²¡æœ‰ä»»ä½•å®é™…ç”¨é€”ã€‚

ä¸‹é¢æ˜¯åŠ è½½ä¸€ä¸ªç©ºç™½çš„ model çš„ä»£ç ç¤ºä¾‹ï¼š
```python
from transformers import BertConfig, TFBertModel

# Building the config
config = BertConfig()

# Building the model from the config
model = TFBertModel(config)
```

### ä½¿ç”¨éç©ºç™½ model

ä½¿ç”¨ pre-trained model ä»¥åŠ transferred model çš„ä»£ç ç¤ºä¾‹å¦‚ä¸‹ï¼š
```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")

# åç»­å°±å¯ä»¥ä½¿ç”¨ classifier å¤„ç†ä»»åŠ¡äº†
```

ä» model card å¯ä»¥æ‰¾åˆ° model çš„è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬ç±»å‹ã€‚

### ä¿å­˜ model

ä½¿ç”¨å¦‚ä¸‹ä»£ç ï¼Œå¯ä»¥å°† model ä¿å­˜åˆ°æœ¬åœ°ï¼š
```python
model.save_pretrained("directory_on_my_computer")
```

æ­¤æ­¥éª¤ä¼šç”Ÿæˆä¸¤ä¸ªæ–‡ä»¶ï¼Œæ¶æ„æè¿° `config.json` ä¸ model å‚æ•°å€¼ï¼Œåè€…çš„æ–‡ä»¶åéšæ¨¡å‹ç±»å‹å˜åŒ–ã€‚è¯¦æƒ…è§"Model Repo ä¸­éƒ½æœ‰ä»€ä¹ˆ"ä¸€èŠ‚ã€‚ 

## ä»»åŠ¡å¤„ç†æµç¨‹

ä¸‹é¢æ˜¯ä¸€ä¸ª NLP Text Classification ä»»åŠ¡ï¼š

<script
	type="module"
	src="https://gradio.s3-us-west-2.amazonaws.com/3.28.3/gradio.js"
></script>

<gradio-app src="https://glrh11-distilbert-base-uncased-finetuned-sst-2-english.hf.space"></gradio-app>

æˆ‘ä»¬çš„è¾“å…¥æ˜¯ä¸€è¡Œæ–‡æœ¬ï¼ˆå¦‚ï¼š"I love you."ï¼‰ï¼Œè¾“å‡ºæ˜¯ä¸€ä¸ªåˆ†ç±»ç»“æœï¼ŒåŒ…å«äº†é¢„æµ‹çš„ç±»åˆ«ï¼ˆPositive/Negativeï¼‰å’Œæ¦‚ç‡ã€‚

å¯¹äºå…¶ä»–ä»»åŠ¡ï¼Œè™½ç„¶è¾“å…¥è¾“å‡ºä¸åŒï¼Œä½†æ˜¯å¤„ç†æµç¨‹æ˜¯ä¸€æ ·çš„ã€‚
![](/assets/image/20230514-huggingface-transformer/full_nlp_pipeline.svg)

### pre-processing

å› ä¸º model ä¸èƒ½ç›´æ¥å¤„ç† text ï¼Œæ‰€ä»¥éœ€è¦å°†æ–‡æœ¬è½¬æ¢æˆæ•°å­—ï¼Œè¿™ä¸ªè¿‡ç¨‹å«åš tokenizeï¼ˆpre-processingä¸­åªåŒ…å«è¿™ä¸€æ­¥ï¼‰ï¼Œtokenize çš„è¿‡ç¨‹æ˜¯ç”± tokenizer å®Œæˆçš„ã€‚ä¸»è¦åšçš„äº‹æƒ…å¦‚ä¸‹ï¼š
1. å°†æ–‡æœ¬åˆ‡åˆ†ä¸º wordã€subwordã€æ ‡ç‚¹ç¬¦å·ç­‰ï¼Œè¿™äº›ç»Ÿç§°ä¸º token
2. å°† token æ˜ å°„ä¸ºæ•°å­—
3. æ·»åŠ ä¸€äº›å¯¹æ¨¡å‹æœ‰å¸®åŠ©çš„é¢å¤–è¾“å…¥

è·å– text çš„ token çš„ä»£ç åŠè¾“å‡ºç¤ºä¾‹ï¼š
```python
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
# ä¸‹é¢çš„æ–¹æ³•ä¼šè‡ªåŠ¨å¯»æ‰¾æ­¤æ¨¡å‹é¢„è®­ç»ƒæ—¶çš„ tokenizer
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="tf")
print(inputs)

# è¾“å‡ºå¦‚ä¸‹ï¼ˆåªå…³æ³¨ input_ids å³å¯ï¼‰
# {
#     'input_ids': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=
#         array([
#             [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,  2026,  2878,  2166,  1012,   102],
#             [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]
#         ], dtype=int32)>, 
#     'attention_mask': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=
#         array([
#             [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
#             [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]
#         ], dtype=int32)>
# }
```

### model-processing

model æ¥å— tokens ä½œä¸ºè¾“å…¥ï¼Œè¾“å‡º hidden statesï¼ˆä¹Ÿç§°ä¸º featuresï¼‰ï¼Œæ­¤ç»“æœéœ€è¦ç»è¿‡ head å±‚ï¼Œæ‰èƒ½è½¬åŒ–ä¸ºåˆ†ç±»åŠåˆ†æ•°ã€‚
model çš„å¤„ç†ç»†èŠ‚å¦‚ä¸‹ï¼š
![](/assets/image/20230514-huggingface-transformer/transformer_and_head.svg)

ç¤ºä¾‹ä»£ç ï¼ˆmodel+head å¤„ç†ï¼‰å¦‚ä¸‹ï¼š
```python
from transformers import TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
# ä¸‹é¢çš„æ–¹æ³•ä¼šæ‰¾åˆ°é¢„è®­ç»ƒé˜¶æ®µçš„ model åŠ headï¼Œä¸åŒçš„ä»»åŠ¡å¯¹åº”ä¸åŒçš„ç±»
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(inputs)
print(outputs.logits)

# è¾“å‡ºå¦‚ä¸‹ï¼ˆä»¥ç¬¬ä¸€ä¸ªå…ƒç´ [-1.5606991,  1.6122842]ä¸ºä¾‹ï¼Œ-1.5606991 æ˜¯ç±»åˆ«1çš„åˆ†æ•°ï¼Œ1.6122842 æ˜¯ç±»åˆ«2çš„åˆ†æ•°ï¼‰
# <tf.Tensor: shape=(2, 2), dtype=float32, numpy=
#     array([[-1.5606991,  1.6122842],
#            [ 4.169231 , -3.3464472]], dtype=float32)>
```

### post-processing

model äº§ç”Ÿçš„ç»“æœï¼Œç”¨æˆ·å¹¶ä¸èƒ½ç›´æ¥å¤„ç†ï¼Œè¿˜éœ€è¦ç»è¿‡ä¸€å±‚å¤„ç†ï¼Œç¤ºä¾‹ä»£ç å¦‚ä¸‹ï¼š
```python
import tensorflow as tf

predictions = tf.math.softmax(outputs.logits, axis=-1)
print(predictions)

# è¾“å‡ºå¦‚ä¸‹ï¼ˆä»¥ç¬¬ä¸€ä¸ªå…ƒç´ [0.04019517, 0.95980483]ä¸ºä¾‹ï¼Œ0.04019517 æ˜¯ç±»åˆ«1çš„æ¦‚ç‡ï¼Œ0.95980483 æ˜¯ç±»åˆ«2çš„æ¦‚ç‡ï¼‰
# tf.Tensor(
# [[4.01951671e-02 9.59804833e-01]
# [9.9945587e-01 5.4418424e-04]], shape=(2, 2), dtype=float32)
```

model label å­˜å‚¨åœ¨ model repo çš„ `config.json` çš„ `id2label` å­—æ®µä¸­ï¼Œç¤ºä¾‹ä»£ç æ¨¡å‹æ­¤å­—æ®µçš„å€¼ä¸º `{0: 'NEGATIVE', 1: 'POSITIVE'}`ï¼Œæ‰€ä»¥æœ€ç»ˆè¾“å‡ºä¸ºï¼š
1. ç¬¬ä¸€ä¸ªå¥å­: `NEGATIVE: 0.0402, POSITIVE: 0.9598`
2. ç¬¬äºŒä¸ªå¥å­: `NEGATIVE: 0.9995, POSITIVE: 0.0005`

### ä½¿ç”¨ pipeline

transformers çš„ `pipeline` æ–¹æ³•ï¼Œå¯ä»¥å°†ä¸Šé¢ä¸‰ä¸ªæ­¥éª¤èšåˆèµ·æ¥ï¼Œæ›´æ–¹é¢ç”¨æˆ·ä½¿ç”¨ï¼Œç¤ºä¾‹å¦‚ä¸‹ï¼š
```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
outputs = classifier(
    [
        "I've been waiting for a HuggingFace course my whole life.",
        "I hate this so much!",
    ]
)

# è¾“å‡ºå¦‚ä¸‹ï¼š
# [{'label': 'POSITIVE', 'score': 0.9598047137260437},
#  {'label': 'NEGATIVE', 'score': 0.9994558095932007}]
```

pipeline ä¼ å…¥çš„å‚æ•°æ˜¯ä»»åŠ¡ç±»å‹ï¼Œç»†èŠ‚å¯ä»¥è‡ªè¡ŒæŸ¥é˜…æ–‡æ¡£ã€‚

## Model Repo ä¸­éƒ½æœ‰ä»€ä¹ˆ

model ä¸­ä¸»è¦åŒ…å«ä¸‰ç±»æ–‡ä»¶ï¼š
1. `config.json`: model çš„æ¶æ„é…ç½®ï¼Œæ¯”å¦‚ model çš„å±‚æ•°ã€hidden sizeã€dropout ç­‰
2. `pytorch_model.bin` / `tf_model.h5` ç­‰: model çš„æƒé‡ï¼Œä¹Ÿå°±æ˜¯ model çš„å‚æ•°å€¼ã€‚ä¸åŒçš„æ¡†æ¶ç”Ÿæˆçš„æ–‡ä»¶ä¸åŒã€‚
3. `README.md`: model çš„ä»‹ç»ï¼ŒåŒ…æ‹¬ model çš„è®­ç»ƒæ•°æ®ã€è®­ç»ƒå‚æ•°ã€è®­ç»ƒç»“æœç­‰ï¼Œmodel card æ˜¯æ ¹æ®æ­¤æ–‡ä»¶ç”Ÿæˆçš„

## å‚è€ƒ

1. [Hugging Face NLP Course](https://huggingface.co/learn/nlp-course/chapter0/1?fw=pt)
