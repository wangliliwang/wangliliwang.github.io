---
layout: post
title:  "ğŸ¤— Hugging Face Model ä½¿ç”¨è¯´æ˜"
date:   2023-05-07 13:11:49 +0800
categories: AI
tags: ["AI"]
toc: true
language: chinese
sidebar: true
author: Wang Li
---

æœ¬æ–‡ç”¨äºä¸äº†è§£transformer modelçš„è¯»è€…å…¥é—¨ä½¿ç”¨ã€‚
ä¸»è¦å†…å®¹ç”¨ transformers è¿™ä¸ª Python åŒ…ï¼Œè¯´æ˜ model å¦‚ä½•äº§ç”Ÿã€ä½¿ç”¨ï¼Œä»¥åŠ Hugging Face model repo ä¸­çš„æ–‡ä»¶ã€‚

## Model æ˜¯ä»€ä¹ˆï¼Ÿ

model æ˜¯ä¸€ä¸ªå¯ä»¥å¤„ç†è¾“å…¥æ•°æ®çš„å‡½æ•°ï¼Œè¾“å…¥æ•°æ®å¯ä»¥æ˜¯å›¾ç‰‡ã€æ–‡æœ¬ã€éŸ³é¢‘ç­‰ï¼Œè¾“å‡ºæ•°æ®å¯ä»¥æ˜¯åˆ†ç±»ã€å›å½’ã€æ–‡æœ¬ç”Ÿæˆç­‰ã€‚
model çš„å‚æ•°æ˜¯åœ¨è®­ç»ƒé˜¶æ®µå­¦ä¹ åˆ°çš„ï¼Œè¿™äº›å‚æ•°å¯ä»¥è¢«ä¿å­˜ï¼Œä»¥ä¾¿åœ¨æ¨ç†ï¼ˆè°ƒç”¨ï¼‰é˜¶æ®µä½¿ç”¨ã€‚

### ç©ºç™½ model

ç©ºç™½çš„ model åªåŒ…å«äº†æ¨¡å‹çš„ç»“æ„ï¼Œå‚æ•°æ˜¯éšæœºç”Ÿæˆçš„ï¼Œè¾“å‡ºä¹Ÿæ˜¯éšæœºçš„ï¼Œæ²¡æœ‰ä»»ä½•å®é™…ç”¨é€”ã€‚

ä¸‹é¢æ˜¯åŠ è½½ä¸€ä¸ªç©ºç™½çš„ model çš„ä»£ç ç¤ºä¾‹ï¼š
```python
from transformers import BertConfig, TFBertModel

# Building the config
config = BertConfig()

# Building the model from the config
model = TFBertModel(config)
```

### pretrained model

pretrained model æ˜¯åœ¨ç©ºç™½ model ä¸Šè®­ç»ƒå‡ºæ¥çš„ï¼Œå¯¹äºç›®å‰æµè¡Œçš„ LLMï¼Œéœ€è¦ä½¿ç”¨å¤§é‡çš„è®­ç»ƒæ–°æ•°æ®ï¼Œæ¶ˆè€—å¤§é‡çš„ç®—åŠ›ã€æ—¶é—´ï¼ˆæ•°å‘¨ä»¥ä¸Šï¼‰ã€‚

æ­¤é˜¶æ®µçš„è®­ç»ƒ

## Model å¤„ç†æµç¨‹

ä¸‹é¢æ˜¯ä¸€ä¸ªNLP Text Classificationä»»åŠ¡ï¼š

<script
	type="module"
	src="https://gradio.s3-us-west-2.amazonaws.com/3.28.3/gradio.js"
></script>

<gradio-app src="https://glrh11-distilbert-base-uncased-finetuned-sst-2-english.hf.space"></gradio-app>

æˆ‘ä»¬çš„è¾“å…¥æ˜¯ä¸€è¡Œæ–‡æœ¬ï¼ˆå¦‚ï¼š"I love you."ï¼‰ï¼Œè¾“å‡ºæ˜¯ä¸€ä¸ªåˆ†ç±»ç»“æœï¼ŒåŒ…å«äº†é¢„æµ‹çš„ç±»åˆ«ï¼ˆPositive/Negativeï¼‰å’Œæ¦‚ç‡ã€‚

å¯¹äºå…¶ä»–ä»»åŠ¡ï¼Œè™½ç„¶è¾“å…¥è¾“å‡ºä¸åŒï¼Œä½†æ˜¯å¤„ç†æµç¨‹æ˜¯ä¸€æ ·çš„ã€‚å¦‚ä¸‹å›¾ï¼š
ï¼ˆå¤„ç†æµç¨‹å›¾ï¼‰

### pre-processing

å› ä¸º model ä¸èƒ½ç›´æ¥å¤„ç† text ï¼Œæ‰€ä»¥éœ€è¦å°†æ–‡æœ¬è½¬æ¢æˆæ•°å­—ï¼Œè¿™ä¸ªè¿‡ç¨‹å«åštokenizeï¼ˆpre-processingä¸­åªåŒ…å«è¿™ä¸€æ­¥ï¼‰ï¼Œtokenize çš„è¿‡ç¨‹æ˜¯ç”± tokenizer å®Œæˆçš„ã€‚ä¸»è¦åšçš„äº‹æƒ…å¦‚ä¸‹ï¼š
1. å°†æ–‡æœ¬åˆ‡åˆ†ä¸ºwordã€subwordã€æ ‡ç‚¹æ ‡ç‚¹ç¬¦å·ç­‰ï¼Œè¿™äº›ç»Ÿç§°ä¸º token
2. å°† token æ˜ å°„ä¸ºæ•°å­—
3. æ·»åŠ ä¸€äº›å¯¹æ¨¡å‹æœ‰å¸®åŠ©çš„é¢å¤–è¾“å…¥

è·å– text çš„ token çš„ä»£ç ç¤ºä¾‹åŠè¾“å‡ºå¦‚ä¸‹ï¼š
```python
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
# ä¸‹é¢çš„æ–¹æ³•ä¼šè‡ªåŠ¨å¯»æ‰¾æ­¤æ¨¡å‹é¢„è®­ç»ƒæ—¶çš„ tokenizer
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="tf")
print(inputs)

# è¾“å‡ºå¦‚ä¸‹ï¼ˆåªå…³æ³¨ input_ids å³å¯ï¼‰
# {
#     'input_ids': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=
#         array([
#             [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607,  2026,  2878,  2166,  1012,   102],
#             [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]
#         ], dtype=int32)>, 
#     'attention_mask': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=
#         array([
#             [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
#             [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]
#         ], dtype=int32)>
# }
```

### model forward

æ¨¡å‹æ¥å— tokens ä½œä¸ºè¾“å…¥ï¼Œè¾“å‡º hidden statesï¼ˆä¹Ÿç§°ä¸º featuresï¼‰ï¼Œæ­¤ç»“æœéœ€è¦ç»è¿‡ head å±‚ï¼Œæ‰èƒ½è½¬åŒ–ä¸ºåˆ†ç±»åŠåˆ†æ•°ã€‚
model çš„å¤„ç†ç»†èŠ‚å¦‚ä¸‹å›¾ï¼š
ï¼ˆtodo å¦ä¸€ä¸ªå¤„ç†å›¾ï¼‰

ç¤ºä¾‹ä»£ç ï¼ˆmodel+headå¤„ç†ï¼‰å¦‚ä¸‹ï¼š
```python
from transformers import TFAutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
# ä¸‹é¢çš„æ–¹æ³•ä¼šæ‰¾åˆ°é¢„è®­ç»ƒé˜¶æ®µçš„ model åŠ headï¼Œä¸åŒçš„ä»»åŠ¡å¯¹åº”ä¸åŒçš„ç±»
model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(inputs)
print(outputs.logits)

# è¾“å‡ºå¦‚ä¸‹ï¼ˆä»¥ç¬¬ä¸€ä¸ªå…ƒç´ [-1.5606991,  1.6122842]ä¸ºä¾‹ï¼Œ-1.5606991 è¡¨ç¤ºç±»åˆ«ï¼Œ1.6122842 è¡¨ç¤ºåˆ†æ•°ï¼‰ï¼š
# <tf.Tensor: shape=(2, 2), dtype=float32, numpy=
#     array([[-1.5606991,  1.6122842],
#            [ 4.169231 , -3.3464472]], dtype=float32)>
```

### post-processing

model äº§ç”Ÿçš„ç»“æœï¼Œå¹¶ä¸èƒ½è¢«ç›´æ¥ç”¨æ¥å±•ç¤ºç»™ç”¨æˆ·ï¼Œè¿˜éœ€è¦ç»è¿‡ä¸€å±‚å¤„ç†ï¼Œç¤ºä¾‹ä»£ç å¦‚ä¸‹ï¼š
```python
import tensorflow as tf

predictions = tf.math.softmax(outputs.logits, axis=-1)
print(predictions)

# è¾“å‡ºå¦‚ä¸‹ï¼ˆä»¥ç¬¬ä¸€ä¸ªå…ƒç´ [0.04019517, 0.95980483]ä¸ºä¾‹ï¼Œ0.04019517 è¡¨ç¤ºç±»åˆ«ï¼Œ0.95980483 è¡¨ç¤ºæ¦‚ç‡ï¼‰ï¼š
# tf.Tensor(
# [[4.01951671e-02 9.59804833e-01]
# [9.9945587e-01 5.4418424e-04]], shape=(2, 2), dtype=float32)
```

ç±»åˆ«çš„ label å­˜å‚¨åœ¨ model repo çš„ `configs.json` çš„ `id2label` å­—æ®µä¸­ï¼Œç¤ºä¾‹ä»£ç æ¨¡å‹æ­¤å­—æ®µçš„å€¼ä¸º `{0: 'NEGATIVE', 1: 'POSITIVE'}`ï¼Œæ‰€ä»¥æœ€ç»ˆè¾“å‡ºä¸ºï¼š
1. ç¬¬ä¸€ä¸ªå¥å­: NEGATIVE: 0.0402, POSITIVE: 0.9598
2. ç¬¬äºŒä¸ªå¥å­: NEGATIVE: 0.9995, POSITIVE: 0.0005

### ä½¿ç”¨ pipeline

transformers çš„ `pipeline` æ–¹æ³•ï¼Œå¯ä»¥å°†ä¸Šé¢ä¸‰ä¸ªæ­¥éª¤èšåˆèµ·æ¥ï¼Œæ›´æ–¹é¢ä½¿ç”¨æ¨¡å‹ï¼Œä½¿ç”¨ç¤ºä¾‹å¦‚ä¸‹ï¼š
```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
outputs = classifier(
    [
        "I've been waiting for a HuggingFace course my whole life.",
        "I hate this so much!",
    ]
)

# è¾“å‡ºå¦‚ä¸‹ï¼š
# [{'label': 'POSITIVE', 'score': 0.9598047137260437},
#  {'label': 'NEGATIVE', 'score': 0.9994558095932007}]
```

pipeline ä¼ å…¥çš„å‚æ•°æ˜¯ä»»åŠ¡ç±»å‹ï¼Œç»†èŠ‚å¯ä»¥è‡ªè¡ŒæŸ¥é˜…æ–‡æ¡£ã€‚

## Model Repo ä¸­éƒ½æœ‰ä»€ä¹ˆ

## å‚è€ƒ

1. [Hugging Face Docs](https://huggingface.co/docs)
